# Week Beginning 1st April 2019
## Monday 1st
* Completed patch to increase debugging for [MB-33423](https://issues.couchbase.com/browse/MB-33423). see [http://review.couchbase.org/#/c/106703](http://review.couchbase.org/#/c/106703/)
* Worked on [MB-33253](https://issues.couchbase.com/browse/MB-33253) appears to be a problem with when we flush a bucket.  Where EP ns_server calls a series of delete vbuckets followed by a series of create vbuckets. The DCP connections seem to remain intact and we do not clean-up the number of active/snoozing backfills correctly.

## Tuesday 2nd
* Contined work on [MB-33253](https://issues.couchbase.com/browse/MB-33253).  After talking with Jim understand the behaviour to be that we want to close all streams on a flush however the dcp connections can remain.  Worked on duplicating the issue with a exp_store_test that does the following:

``` 
1. Creates some items
2. Creates a new checkpoint
3. Create more items
4. Create another checkpoint - (now can remove initial checkpoint)
5. Remove closed unreferenced checkpoints
   (will result in producer requiring a backfill)
6. Create a DCP producer
7. Do a streamRequest of the producer to produce a stream.
   (This will result in the backfill being generated.)
8. Delete the vbucket
9. Re-create the vbucket - step 8 and 9 replicates what happens on a flush.
10. Repeat steps 1 to 5 so that another backfill will be required.
11. Repeat step 7.
```

## Wednesday 3rd
* Contined work on [MB-33253](https://issues.couchbase.com/browse/MB-33253).  Got test working, however it looks like although we end-up running backfills for dead stream, they do complete in the end.  So I still don't understand why we are seeing the issue reported.  Therefore worked on trying to reproduce the test.  Had issue:
```
godeps/src/github.com/andelf/go-curl/easy.go:408:22: identifier
"_Ctype_struct_curl_slist" may conflict with identifiers generated by    cgo
```
Ben Brooks told me that he thought was a `vet` tool warning (newer versions of Go won't compile when there are vet warnings). And that he tought GSI used go 1.7.6. and that different version of go can be downloaded from [here](https://golang.org/doc/install#extra_versions)

* Worked on [CBSE-6603](https://issues.couchbase.com/browse/CBSE-6603) where we are seeing rebalance failures with no_stats_for_this_vbucket.  The failures are due to connections being disconnected due to NOOP timeouts.  Also seeing very slow GET_REPLICA calls (some taking over one hour!).  There appears to be slow storage with slow FlusherTask times, but what confused me is that the test show it is 100% resident.  I think it may be due to backfill that is occuring during backfill.  The reason for this is the slow GET_REPLICA calls are only seen in batches of time and not all the time.  Need to investigate further.

## Thursday 4th
* Continued work on [CBSE-6603](https://issues.couchbase.com/browse/CBSE-6603).  Initially thought the slow GET_REPLICAs was due to slow disks however discovered the issue is to do with the request coming to a vbucket in pending state.  This means that the operation is put into pending operations.  We wake-up to a task to process the pending operations when moving to an active state - but not when moving to a replica state - see [here](http://src.couchbase.org/source/xref/4.6.5/ep-engine/src/ep.cc#1177).  So when we move from pending to replica we do nothing.  Later on we go back to a pending state and then to active at which point we process the pending operations, however significant time has past.

It is worth noting that the rebalance failures will not be effected by the slow GET_REPLICAS.  Instead they are occuring becuase of lack of NOOP detection.

__Actions__

* Follow-up with ns_server team why we are moving from pending to replica.  Also check with them why we are remaining in pending for so long (in the CBSE we are seeing ~3 minutes).  It is supposed to be a very short period of time.

## Friday 5th

* Went back to work on [MB-33253](https://issues.couchbase.com/browse/MB-33253).  Got the go tests to run and pass repeatedly, so went back to spend signficant time analysing the logs in the MB and reviewing the code in detail.  After not identity any cause, I decided to do some archeology
on the code to look for any recent changes, which would explain why I was
seeing the test pass.  Uncovered a recent patch that was committed in [http://review.couchbase.org/#/c/104988/](http://review.couchbase.org/#/c/104988/) on Feburary 18th.  I suspect that the issue was rasied on a build that did not contain this patch.  Therefore have advised they rerun the test on the current master to see if it does (as it does for me). 
