# Week Beginning 8th April 2019
## Monday 8th

* Worked on [MB-33683](https://issues.couchbase.com/browse/MB-33683) by learning rebalance behaviour in more detail.  Watched the rebalance [video](https://drive.google.com/open?id=0B6AAZxJB1JWOdFVFcXZRb1FvWmM) from Poonam.  Note:  Slides available from Paolo on email sent 11/9/18. Also updated the MB asking ns_server team why we go from pending to replica.
* Returned to work on expel code.  Focused on test case where we have the follow checkpoint / cursor set-up.  The interesting point is that we have two cursor both pointing to the same seqno (one pointing to a mutation and on pointing to a metadata), and the cursor pointing to the metadata is **after** the cursor pointing to the mutation.

```
1000 - dummy item
1001 - checkpoint start
1001 - set VB state
1001 - mutation  <<<<<<< dcpCursor2
1001 - checkpoint end  <<<<<<< dcpCursor1

1001 - dummy item
1002 - checkpoint start
1002 - mutation   <<<<<<< persistenceCursor
``` 

In this case we should not be able to expel any items however the current code allows us to expel 4 items (dummy, checkpoint_start, set VB state, and mutation). However the shared pointier of the dummy was set to where the mutation was and so it will not be expelled (as it will still have a reference).

Also why developing the test case came across a potential bug in checkpoint, where we have seqno out of order in a checkpoint.

```
checkpoint = CheckpointManager[0x10e209900] with numItems:6 checkpoints:2
    Checkpoint[0x10e209a80] with seqno:{1001,1001} state:CHECKPOINT_CLOSED items:[
        {1000,empty,cid:0x1:empty,102,[m]}
        {1001,checkpoint_start,cid:0x1:checkpoint_start,113,[m]}
        {1001,mutation,cid:0x0:key0,101,}
        {1002,set_vbucket_state,cid:0x1:set_vbucket_state,114,[m]}
        {1001,checkpoint_end,cid:0x1:checkpoint_end,111,[m]}
]
    Checkpoint[0x10e209600] with seqno:{1002,1002} state:CHECKPOINT_OPEN items:[
        {1001,empty,cid:0x1:empty,102,[m]}
        {1002,checkpoint_start,cid:0x1:checkpoint_start,113,[m]}
        {1002,mutation,cid:0x0:key1,101,}
]
```

The above state was generated by having checkpoints contain a maximum of 1 mutation and then:

1. Adding an item
2. Adding a set VB state
3. Adding another item

###Actions

* Address the accounting for number of items expelled
* Address the issue of expelling where metadata item is after the mutation, but have the same seqno. - **DONE**
* Investigate the potential bug with Checkpoint.

## Tuesday 9th

* Worked on [MB-33683](https://issues.couchbase.com/browse/MB-33683) following reply from ns_server team.  Learnt that we transition from pending state to replica when a rebalance fails.  Created a cluster that had only 2 vbuckets by doing the following:

```
max_num_shards = 1 // this is a config parameter
export COUCHBASE_NUM_VBUCKETS=2; ./cluster_run -n 2
```

Created a bucket on one of the nodes and then rebalanced the other node in.  This caused vbucket-1 on the initial node to be moved to the newly added node.  From this discovered that ns_server sets a vbucket to a pending state when it is created.  This make the KV setting to pending during the takeover stage redundant (although not sure about in other cases).  Also means that we can be in a pending state for a significant amount of time.  Therefore we will need the fix of returning NOT_MY_VBUCKET for the case of a GET_REPLICA request to a pending vbucket.

* Worked on expel code.  Created test for when a metadata item appears  after a mutation with the same seqno id.  See test called doNotExpelIfHaveMetaAfterMutation.  Created a [patch](http://review.couchbase.org/#/c/107546/) for the test and associated changes to the expel code.  The code has been changed to arrive at the expelPoint by  applying a series of lambda functions.  Realised that I could simplify the code if I applied the findLowestSeqno function first followed by the other lambda functions.  This should allow the other lamda functions (such as moving before metaitems) to be applied to a single possibleExpelPoint as opposed to a list of them.

###Actions

* Raise patch for returning NOT\_MY\_VBUCKET in the case of GET_REPLICA going to a pending vbucket. - **DONE**
* Rework the expel patch so the findLowestSeqno function is applied first. - **DONE**

## Wednesday 10th
* Worked on [MB-33683](https://issues.couchbase.com/browse/MB-33683) replied back to ns\_server team requesting that we not create a vbucket in a pending state prior to the backilling phase and persistence of the high seqno.  Raised [MB-33734](https://issues.couchbase.com/browse/MB-33734) to track the request.
* Implemented the [patch](http://review.couchbase.org/#/c/107600/) for returning NOT\_MY\_VBUCKET in the case of the GET_REPLICA going to a pending vbucket.
* Implemented the expel [patch](http://review.couchbase.org/#/c/107600/) for addressing the issue of expelling where metadata item is after the mutation, but have the same seqno.  Once the expel stat is in place I think we are in a position to re-enable the expel functionality.

## Thursday 11th
* Fixed test on [patch](http://review.couchbase.org/#/c/107600/) where get replica return state from pending was being tested.  Put up ready for review.
* Spent the rest of the day trying help out on [CBSE-6535](https://issues.couchbase.com/browse/CBSE-6535) where start seqno is out of range.  Unfortunately made no real progress may be put down to hardware/memory corruption.
* Starting work on patch for expel stats.
* Sent note to Shivani regarding the differences between the 2bitLRU policy and the HifiMFU policy.

### Learning Points

* Can look at the **Showfast** performace graph [here](http://showfast.sc.couchbase.com/daily)

## Friday 10th
* Got corrected that for a backfill we do not put the contents into the hash table/cached - had to update Shivani of the correction.  We do not put into the cache for two reasons:
 1. It is likely the cache would be polluted with cold items that just need to be transferred to a DCP consumer (i.e. imagine the case of a stream to a backup client)
 2. It gives DCP streams control of what is kept resident in the cache.
* Fixed comment with NOT\_MY\_VBUCKET [patch](http://review.couchbase.org/#/c/107600/).  Code got merged.  Need to backport to 6.0.2 see **Actions** at the end.
* Raised [MB-33768](https://issues.couchbase.com/browse/MB-33768) for adding expel stat.  Also got [patch](http://review.couchbase.org/#/c/107713) ready for review.
* Replied back on [MB-33734](https://issues.couchbase.com/browse/MB-33734) that want to investigate the performance of setting bucket state to replica instead of pending at the start of a vbucket move.  DaveF sent the ns\_server patch for making the change so I can perform the investigation.
* Ben showed me how to use perf c2c.
* Took over DaveR's checkpoint [patch](http://review.couchbase.org/#/c/103548/), which does a couple of things:
 1. Check item has a seqno within the extent of the checkpoint at the start of Checkpoint::queueDirty.
 2. Fix getLowSeqno and getHighSeqno so do not crash when checkpoint is empty.
It appears in gerrit you cannot change owner, so I spawned my own version of the oatc, [here](http://review.couchbase.org/#/c/107720/)

###Learning Points

* How to use perf c2c.  First set-off test, then login to Triton.  Need to install the debug symbols for the build that we are testing.  This can be achieved doing the following:

```
wget http://172.23.120.24/builds/latestbuilds/couchbase-server/mad-hatter/2715/couchbase-server-enterprise-debuginfo-6.5.0-2715-centos7.x86_64.rpm
```
followed by:

```
 rpm -Uvh couchbase-server-enterprise-debuginfo-6.5.0-2715-centos7.x86_64.rpm 
```
To run the perf c2c an use ```history``` command to see previous invocations.  It is a 2 stage process.  First phase is to implement capture phase using ```perf c2c record``` e.g. by running a command such as:

```
perf c2c record -p $(pgrep memcached) -a --all-user --per-thread -F 200 -o c2ccg.data --call-graph=dwarf,8192 
```
This captures a call graph can also capture without the call graph:

```
 perf c2c record -p $(pgrep memcached) -a --all-user --per-thread -F 200 -o c2c.data
```
10 seconds worth of capture is usually sufficient and therefore issue a command such as:

```
perf c2c record -p $(pgrep memcached) -a --all-user --per-thread -F 200 -o c2ccg.data --call-graph=dwarf,8192  sleep 10 && perf c2c record -p $(pgrep memcached) -a --all-user --per-thread -F 200 -o c2c.data sleep 10
```
After the record phase we have the reporting phase.  This is done as follows:

```
perf c2c report --full-symbols --coalesce iaddr,dso --node-info --stdio --call-graph graph,0.5,callee,function,period -i c2ccg.data
```
and for the non-call-graph version:

```
perf c2c report --full-symbols --coalesce iaddr,dso --node-info --stdio -i c2c.data
```

###Actions

* Back port GET_REPLICA fix to 6.0.2. - see [MB-33683](https://issues.couchbase.com/browse/MB-33683)
* Discuss with DaveR adding the extent check at the start of Checkpoint::queueDirty.  As don't believe his initial statement that "An equivalent check was previously done in CheckpointManager::queueDirty before calling Checkpoint::queueDirty()" is correct.
* Re: [MB-33734](https://issues.couchbase.com/browse/MB-33734) do performance investigation on the impact of moving a vbucket into a replica state as opposed to a pending state at the start of a vbucket move.